~/src/mddg ~/src/mddg
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 34663.67item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:28<01:26, 28.74s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:00<01:01, 30.66s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:33<00:31, 31.48s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:39<00:00, 21.52s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:39<00:00, 24.86s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 77.27037644386292 seconds
Total output tokens: 109
Avg out tokens per prompt: 54.5
model_output/documents/meta-llama/Meta-LLaMA-3-8B/toy/zero-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 37786.52item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:27<01:22, 27.42s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:58<00:59, 29.71s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:28<00:29, 29.65s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:33<00:00, 20.14s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:33<00:00, 23.47s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 9.410086393356323 seconds
Total output tokens: 1078
Avg out tokens per prompt: 539.0
model_output/documents/meta-llama/Meta-LLaMA-3-8B-Instruct/toy/zero-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 61680.94item/s]
2 examples.

Concurrently invoking GPT-4O:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-4O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 92182.51prompt/s]
[1mConcurrent executed 2 examples in 0.54 seconds.[0m
model_output/documents/gpt/gpt-4o/toy/zero-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 58661.59item/s]
2 examples.

Concurrently invoking GPT-35-LONG:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-35-LONG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 96420.78prompt/s]
[1mConcurrent executed 2 examples in 0.55 seconds.[0m
model_output/documents/gpt/gpt-35-long/toy/zero-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 28926.23item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:46<02:20, 46.99s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:24<01:23, 41.56s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:00<00:38, 38.76s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:06<00:00, 25.85s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:06<00:00, 31.56s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 93.10454368591309 seconds
Total output tokens: 1004
Avg out tokens per prompt: 502.0
model_output/documents/meta-llama/Meta-LLaMA-3-8B/toy/zero-shot-CoT_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27869.13item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:27<01:22, 27.56s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:54<00:54, 27.45s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:21<00:27, 27.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:26<00:00, 18.43s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:26<00:00, 21.71s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 92.64993834495544 seconds
Total output tokens: 3440
Avg out tokens per prompt: 1720.0
model_output/documents/meta-llama/Meta-LLaMA-3-8B-Instruct/toy/zero-shot-CoT_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 59074.70item/s]
2 examples.

Concurrently invoking GPT-4O:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-4O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 91180.52prompt/s]
[1mConcurrent executed 2 examples in 0.42 seconds.[0m
model_output/documents/gpt/gpt-4o/toy/zero-shot-CoT_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 59493.67item/s]
2 examples.

Concurrently invoking GPT-35-LONG:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-35-LONG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 85598.04prompt/s]
[1mConcurrent executed 2 examples in 0.39 seconds.[0m
model_output/documents/gpt/gpt-35-long/toy/zero-shot-CoT_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 21399.51item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:27<01:23, 27.97s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:55<00:55, 27.78s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:23<00:27, 27.65s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:28<00:00, 18.98s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:28<00:00, 22.20s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 92.94207835197449 seconds
Total output tokens: 20
Avg out tokens per prompt: 10.0
model_output/documents/meta-llama/Meta-LLaMA-3-8B/toy/few-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24966.10item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:29<01:29, 29.92s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:01<01:01, 30.91s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:30<00:29, 29.83s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:35<00:00, 20.13s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:35<00:00, 23.83s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 13.513622045516968 seconds
Total output tokens: 341
Avg out tokens per prompt: 170.5
model_output/documents/meta-llama/Meta-LLaMA-3-8B-Instruct/toy/few-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27324.46item/s]
2 examples.

Concurrently invoking GPT-4O:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-4O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 81442.80prompt/s]
[1mConcurrent executed 2 examples in 0.55 seconds.[0m
model_output/documents/gpt/gpt-4o/toy/few-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24892.01item/s]
2 examples.

Concurrently invoking GPT-35-LONG:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-35-LONG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 89240.51prompt/s]
[1mConcurrent executed 2 examples in 0.45 seconds.[0m
model_output/documents/gpt/gpt-35-long/toy/few-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 42799.02item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:30<01:32, 30.80s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:59<00:59, 29.75s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:27<00:28, 28.72s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:33<00:00, 19.64s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:33<00:00, 23.26s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 67.68018531799316 seconds
Total output tokens: 20
Avg out tokens per prompt: 10.0
model_output/table/meta-llama/Meta-LLaMA-3-8B/toy/zero-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 56679.78item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:30<01:32, 30.81s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:03<01:04, 32.01s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:31<00:30, 30.02s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:37<00:00, 20.59s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:37<00:00, 24.36s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 67.83777618408203 seconds
Total output tokens: 1147
Avg out tokens per prompt: 573.5
model_output/table/meta-llama/Meta-LLaMA-3-8B-Instruct/toy/zero-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 59074.70item/s]
2 examples.

Concurrently invoking GPT-4O:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-4O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 85598.04prompt/s]
[1mConcurrent executed 2 examples in 0.51 seconds.[0m
model_output/table/gpt/gpt-4o/toy/zero-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 58254.22item/s]
2 examples.

Concurrently invoking GPT-35-LONG:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-35-LONG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 86480.49prompt/s]
[1mConcurrent executed 2 examples in 0.47 seconds.[0m
model_output/table/gpt/gpt-35-long/toy/zero-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 41527.76item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:31<01:33, 31.27s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:06<01:07, 33.50s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:38<00:33, 33.12s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:45<00:00, 22.42s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:45<00:00, 26.26s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 68.03188586235046 seconds
Total output tokens: 101
Avg out tokens per prompt: 50.5
model_output/table/meta-llama/Meta-LLaMA-3-8B/toy/zero-shot-CoT_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 43240.25item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:56<02:48, 56.10s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:33<01:29, 44.94s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:25<00:48, 48.25s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:34<00:00, 32.73s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:34<00:00, 38.59s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 69.22438836097717 seconds
Total output tokens: 3009
Avg out tokens per prompt: 1504.5
model_output/table/meta-llama/Meta-LLaMA-3-8B-Instruct/toy/zero-shot-CoT_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 58254.22item/s]
2 examples.

Concurrently invoking GPT-4O:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-4O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 89240.51prompt/s]
[1mConcurrent executed 2 examples in 38.53 seconds.[0m
model_output/table/gpt/gpt-4o/toy/zero-shot-CoT_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 59918.63item/s]
2 examples.

Concurrently invoking GPT-35-LONG:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-35-LONG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 82241.25prompt/s]
[1mConcurrent executed 2 examples in 10.21 seconds.[0m
model_output/table/gpt/gpt-35-long/toy/zero-shot-CoT_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 36314.32item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:25<01:16, 25.63s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:54<00:55, 27.60s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:21<00:27, 27.22s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:26<00:00, 18.39s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:26<00:00, 21.56s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 67.86788988113403 seconds
Total output tokens: 17
Avg out tokens per prompt: 8.5
model_output/table/meta-llama/Meta-LLaMA-3-8B/toy/few-shot_output.json
Running open source model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 36954.22item/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Initializing pipeline...

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:24<01:14, 24.82s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:51<00:51, 25.98s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:20<00:27, 27.10s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:25<00:00, 18.37s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:25<00:00, 21.26s/it]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running pipeline...
First prompt: 2.4206082820892334 seconds
Total output tokens: 273
Avg out tokens per prompt: 136.5
model_output/table/meta-llama/Meta-LLaMA-3-8B-Instruct/toy/few-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 38479.85item/s]
2 examples.

Concurrently invoking GPT-4O:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-4O: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 91180.52prompt/s]
[1mConcurrent executed 2 examples in 3.05 seconds.[0m
model_output/table/gpt/gpt-4o/toy/few-shot_output.json
Running GPT model
Processing dataset...

Processing Dataset:   0%|          | 0/2 [00:00<?, ?item/s]
Processing Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 39199.10item/s]
2 examples.

Concurrently invoking GPT-35-LONG:   0%|          | 0/2 [00:00<?, ?prompt/s]
Concurrently invoking GPT-35-LONG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 88301.14prompt/s]
[1mConcurrent executed 2 examples in 4.65 seconds.[0m
model_output/table/gpt/gpt-35-long/toy/few-shot_output.json
